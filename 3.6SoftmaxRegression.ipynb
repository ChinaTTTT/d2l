{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'alse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01md2l\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m torch \u001b[38;5;28;01mas\u001b[39;00m d2l\n\u001b[0;32m      7\u001b[0m d2l\u001b[38;5;241m.\u001b[39muse_svg_display()\n\u001b[0;32m      9\u001b[0m trans \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()\n",
      "File \u001b[1;32me:\\task\\.venv\\Lib\\site-packages\\d2l\\torch.py:1149\u001b[0m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattention_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1147\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m-> 1149\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mMultiHeadAttention\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43md2l\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModule\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"Multi-head attention.\u001b[39;49;00m\n\u001b[0;32m   1151\u001b[0m \n\u001b[0;32m   1152\u001b[0m \u001b[38;5;124;43;03m    Defined in :numref:`sec_multihead-attention`\"\"\"\u001b[39;49;00m\n\u001b[0;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_hiddens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32me:\\task\\.venv\\Lib\\site-packages\\d2l\\torch.py:1153\u001b[0m, in \u001b[0;36mMultiHeadAttention\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMultiHeadAttention\u001b[39;00m(d2l\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m   1150\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Multi-head attention.\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m \n\u001b[0;32m   1152\u001b[0m \u001b[38;5;124;03m    Defined in :numref:`sec_multihead-attention`\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1153\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_hiddens, num_heads, dropout, bias\u001b[38;5;241m=\u001b[39m \u001b[43malse\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1154\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m   1155\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m=\u001b[39m num_heads\n",
      "\u001b[1;31mNameError\u001b[0m: name 'alse' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from d2l import torch as d2l\n",
    "\n",
    "d2l.use_svg_display()\n",
    "\n",
    "trans = transforms.ToTensor()\n",
    "mnist_train = torchvision.datasets.FashionMNIST(\n",
    "    root=\"..\\data\", transform=trans, train=True, download=True \n",
    ")\n",
    "mnist_test = torchvision.datasets.FashionMNIST(\n",
    "    root=\"..\\data\", transform=trans, train=False, download=True\n",
    ")\n",
    "len(mnist_test), len(mnist_train)\n",
    "#mnist第一维存储tensor化的图片，第二维存标签\n",
    "mnist_train.data.shape\n",
    "count_list = [0] * 10\n",
    "for i in range(1000):\n",
    "    a = mnist_train[i][1]\n",
    "    count_list[a] += 1\n",
    "\n",
    "print(count_list) \n",
    "    \n",
    "\n",
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = [\"t-shirt\",\"trouser\",\"pullover\",\"dress\",\"coat\",\"sandal\",\"shirt\",\"sneaker\",\"bag\",\"ankle boot\"]\n",
    "    return [text_labels[int(i)] for i in labels]\n",
    "\n",
    "get_fashion_mnist_labels(range(10))\n",
    "def show_images(imgs, num_rows, num_cols, titles=None, scale = 1.3):\n",
    "    figsize = (num_rows * scale, num_cols * scale)\n",
    "    _ ,axes = d2l.plt.subplots(num_rows, num_cols, figsize = figsize) # #figure对象和 axes数组（包含多个子图）\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate (zip(axes, imgs)):\n",
    "        if torch.is_tensor(img):\n",
    "            ax.imshow(img.numpy())\n",
    "        else:\n",
    "            ax.imshow(img)\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    return axes\n",
    "X, y = next(iter(data.DataLoader(mnist_train, batch_size=20)))\n",
    "show_images(X.reshape(20,28,28), 4, 5, titles=get_fashion_mnist_labels(y))\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "def get_dataloader_workers():#4个worker比1个慢\n",
    "    return 2\n",
    "\n",
    "train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers())\n",
    "\n",
    "timer = d2l.Timer()\n",
    "for X, y in train_iter:\n",
    "    continue\n",
    "f\"{timer.stop():.2f}sec\"\n",
    "\n",
    "#整合组件\n",
    "def load_data_fashion_mnist(batch_size, resize=None):\n",
    "    trans = [transforms.ToTensor()]#转换为tensor\n",
    "    if resize: #调整尺寸\n",
    "        trans.insert(0, transforms.Resize(resize))#插入到第一个位置\n",
    "    trans = transforms.Compose(trans)#组合\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(\n",
    "    root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(\n",
    "    root=\"../data\", train=False, transform=trans, download=True)\n",
    "    \n",
    "    return(data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "            num_workers=get_dataloader_workers()),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "            num_workers=get_dataloader_workers()))\n",
    "\n",
    "train_iter, test_iter = load_data_fashion_mnist(32, resize=64)\n",
    "for X, y in train_iter:\n",
    "    print(X.shape, X.dtype, y.shape, y.dtype)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display   #import display from IPython\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "import pdb\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "W = torch.normal(0,0.01,size=(num_inputs, num_outputs),requires_grad=True)\n",
    "b = torch.zeros(num_outputs, requires_grad=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(1, keepdim=True)#1表示各列即各个特征累积\n",
    "    X_prob = X_exp / partition #X_exp是n条m维的， partition是n条1维的\n",
    "\n",
    "def net(X):\n",
    "    return softmax(torch.matmul(X.reshape(-1, W.shape[0]), W) + b) #输出为y_hat n条10维的\n",
    "\n",
    "def cross_entropy(y_hat, y):\n",
    "    return - torch.log(y_hat [range(len(y_hat)),y])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "def evaluate_accuracy(net, data_iter):\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()\n",
    "    metric = Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            metric.add(accuracy(net(X), y), y.numel)#将准确率和元素个数添加进metric\n",
    "    return metric[0] / metric[1] #accuracy / elements num\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[1;32m---> 17\u001b[0m \u001b[43mevaluate_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[97], line 23\u001b[0m, in \u001b[0;36mevaluate_accuracy\u001b[1;34m(net, data_iter)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m data_iter:\n\u001b[1;32m---> 23\u001b[0m         metric\u001b[38;5;241m.\u001b[39madd(\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mnumel)\u001b[38;5;66;03m#将准确率和元素个数添加进metric\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metric[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m metric[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[96], line 2\u001b[0m, in \u001b[0;36maccuracy\u001b[1;34m(y_hat, y)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccuracy\u001b[39m(y_hat, y):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43my_hat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_hat\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m      3\u001b[0m         y_hat \u001b[38;5;241m=\u001b[39m y_hat\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m     cmp \u001b[38;5;241m=\u001b[39m y_hat\u001b[38;5;241m.\u001b[39mtype(y\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m==\u001b[39m y\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "class Accumulator:\n",
    "\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args) ]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "\n",
    "evaluate_accuracy(net, test_iter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 784]' is invalid for input of size 131072",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[95], line 7\u001b[0m, in \u001b[0;36mnet\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnet\u001b[39m(X):\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m softmax(torch\u001b[38;5;241m.\u001b[39mmatmul(\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, W) \u001b[38;5;241m+\u001b[39m b)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 784]' is invalid for input of size 131072"
     ]
    }
   ],
   "source": [
    "net(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
